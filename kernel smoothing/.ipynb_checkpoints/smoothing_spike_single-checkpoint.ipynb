{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{L}[1]{\\| #1 \\|}\\newcommand{VL}[1]{\\L{ \\vec{#1} }}\\newcommand{R}[1]{\\operatorname{Re}\\,(#1)}\\newcommand{I}[1]{\\operatorname{Im}\\, (#1)}$\n",
    "\n",
    "## An introduction to smoothing\n",
    "\n",
    "Smoothing is a process by which data points are averaged with their neighbors\n",
    "in a series, such as a time series, or image. This (usually) has the effect of\n",
    "blurring the sharp edges in the smoothed data.  Smoothing is sometimes\n",
    "referred to as filtering, because smoothing has the effect of suppressing high\n",
    "frequency signal and enhancing low frequency signal. There are many different\n",
    "methods of smoothing, but here we discuss smoothing with a Gaussian kernel. We\n",
    "hope we will succeed in explaining this phrase in the explanation below.\n",
    "\n",
    "### Some example data for smoothing\n",
    "\n",
    "First we load and configure the libraries we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Make numpy print 4 significant digits for prettiness\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "np.random.seed(5) # To get predictable random numbers\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.animation as animation\n",
    "from sklearn import datasets  # to retrieve the iris Dataset\n",
    "import pandas as pd  # to load the dataframe\n",
    "from sklearn.preprocessing import StandardScaler  # to standardize the features\n",
    "from sklearn.decomposition import PCA  # to apply PCA\n",
    "import seaborn as sns  # to plot the heat maps#float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a set of data, made out of random numbers, that we will use as a\n",
    "pretend time series, or a single line of data from one plane of an\n",
    "image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/spikes_v1_clean.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_301749/1423214730.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load spike data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mspike_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/spikes_v1_clean.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mspike_data_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspike_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m999\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspike_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/brain/lib/python3.9/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    415\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/spikes_v1_clean.npy'"
     ]
    }
   ],
   "source": [
    "# Load spike data\n",
    "spike_data = np.load('data/spikes_v1_clean.npy')                      \n",
    "spike_data_t = spike_data[:,0:999]\n",
    "print(np.shape(spike_data))\n",
    "\n",
    "# short time span\n",
    "t = np.shape(spike_data)[1]\n",
    "n = np.shape(spike_data)[0]\n",
    "\n",
    "scalar = StandardScaler()\n",
    "scaled_data = pd.DataFrame(scalar.fit_transform(spike_data)) #scaling the data\n",
    "print(scaled_data.shape)\n",
    "\n",
    "\n",
    "\n",
    "# scaled data or non scaled data\n",
    "length = 1000\n",
    "nn = 1\n",
    "y_spike_full = pd.DataFrame.to_numpy(scaled_data)\n",
    "\n",
    "y_spike = spike_data[nn-1,0:length]\n",
    "t_full = np.linspace(1, t, t)\n",
    "t_x = np.arange(length)\n",
    "n_y = np.shape(y_spike)[0]\n",
    "\n",
    "plt.bar(t_x,y_spike)\n",
    "#print(t_x)\n",
    "#print(y_spike[nn-1,0:length])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Gaussian kernel\n",
    "\n",
    "The ‘kernel’ for smoothing, defines the shape of the function that is\n",
    "used to take the average of the neighboring points. A Gaussian kernel\n",
    "is a kernel with the shape of a Gaussian (normal distribution) curve.\n",
    "Here is a standard Gaussian, with a mean of 0 and a $\\sigma$ (=population\n",
    "standard deviation) of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-6, 6, 0.1) # x from -6 to 6 in steps of 0.1\n",
    "sigma = 2\n",
    "y = 1 / (sigma*np.sqrt(2 * np.pi)) * np.exp(-x ** 2 / 2*sigma**2)\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the standard statistical way, we have defined the width of the Gaussian\n",
    "shape in terms of $\\sigma$. However, when the Gaussian is used for smoothing,\n",
    "it is common for imagers to describe the width of the Gaussian with another\n",
    "related measure, the Full Width at Half Maximum (FWHM).\n",
    "\n",
    "The FWHM is the width of the kernel, at half of the maximum of the\n",
    "height of the Gaussian. Thus, for the standard Gaussian above, the\n",
    "maximum height is ~0.4.  The width of the kernel at 0.2 (on the Y axis) is the\n",
    "FWHM. As x = -1.175 and 1.175 when y = 0.2, the FWHM is roughly 2.35.\n",
    "\n",
    "The FWHM is related to sigma by the following formulae (in Python):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma2fwhm(sigma):\n",
    "    return sigma * np.sqrt(8 * np.log(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fwhm2sigma(fwhm):\n",
    "    return fwhm / np.sqrt(8 * np.log(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma2fwhm(sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smoothing with the kernel\n",
    "\n",
    "The basic process of smoothing is very simple. We proceed through the\n",
    "data point by point. For each data point we generate a new value that is\n",
    "some function of the original value at that point and the surrounding\n",
    "data points.With Gaussian smoothing, the function that is used is our\n",
    "Gaussian curve..\n",
    "\n",
    "So, let us say that we are generating the new, smoothed value for the\n",
    "14th value in our example data set. We are using a Gaussian with FWHM of\n",
    "4 units on the x axis. To generate the Gaussian kernel average for this\n",
    "14th data point, we first move the Gaussian shape to have its center at\n",
    "13 on the x axis (13 is the 14th value because the first value is 0). In order\n",
    "to make sure that we don’t do an overall scaling of the values after\n",
    "smoothing, we divide the values in the Gaussian curve by the total area under\n",
    "the curve, so that the values add up to one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "FWHM = 20\n",
    "sigma = fwhm2sigma(FWHM)\n",
    "x_position = 100 # 14th point\n",
    "kernel_at_pos_n = np.exp(-(t_x - x_position) ** 2 / (2 * sigma ** 2))\n",
    "kernel_at_pos_n = kernel_at_pos_n / sum(kernel_at_pos_n)\n",
    "plt.bar(t_x, kernel_at_pos_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact the Gaussian values for the 12th through 16th data points are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the data values for the same points are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then multiply the Gaussian kernel (weight) values by the values of our\n",
    "data, and sum the results to get the new smoothed value for point 13:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "y_spike_by_weight = y_spike * kernel_at_pos_n # element-wise multiplication\n",
    "new_val_spike = sum(y_spike_by_weight)\n",
    "new_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We store this new smoothed value for future use, and move on, to x = 14,\n",
    "and repeat the process, with the Gaussian kernel now centered over 14.  If we\n",
    "do this for each point, we eventually get the smoothed version of our original\n",
    "data. Here is a very inefficient but simple way of doing this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "smoothed_vals_spike = np.zeros(y_spike.shape)\n",
    "\n",
    "for x_position in t_x:\n",
    "    #print(x_position)\n",
    "    kernel = np.exp(-(t_x - x_position) ** 2 / (2 * sigma ** 2))\n",
    "    kernel = kernel / sum(kernel)\n",
    "    smoothed_vals_spike[x_position] = sum(y_spike * kernel)\n",
    "    print(x_posiiton)\n",
    "plt.bar(t_x, smoothed_vals_spike)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(smoothed_vals_spike)\n",
    "\n",
    "ax.set(xlabel='time (s)', ylabel='spike',\n",
    "       title='per neuron')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
